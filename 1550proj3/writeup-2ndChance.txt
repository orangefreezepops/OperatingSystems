To find the page fault statistics I tested the suggested
memory configurations 1:1, 1:3, 3:1, 3:5, 5:3, 7:9 and 9:7 
with 8 and 1024 page frames and page sizes 4KB and 4MB
on the given 1.trace file. (With the exception that I did not use
memory split 7:9 and 9:7 for the 8 frame runs.) Attached I have 
also included tables for the simulation runs along with line
graph representations of the data.

Overall the traces behaved as expected, with the higher the 
number of page frames, the lower number of page faults and disk writes.
Page faults tended to remain steady for the 1024 page configurations 
and differed only by 1 page fault in a singular case 
(configuration 1024 frames, 4KB pages and 1:3 frame split).
For the 8 page frame run the faults were more dicey and could vary
thousands, as could the disk writes.

The writes to disk in the overall trend of the runs had more 
variance, and I believe that was a product of the uneven dispersion
of instructions for certain processes along with the memory split
configurations. (e.g. if process 0 receives the 1 in a 1:3 split and 
has say 25% more store instructions than process 1, process 0 is 
storing more addresses and writing more frequently after page 
evictions, compound that with the fact that it received a smaller 
share of the frames, and now your disk writes will spike.)

The most glaring datapoint I found was in the 8 page frame, 4MB 
page size run with a 1:3 memory split. If you look at page 3 of my 
metrics pdf, you'll notice an enormous spike in both page faults and
disk writes as compared to the other memory splits of the same configuartion.
I don't think this is an example of Belady's anomaly, because it wasn't
an increase in the number of frames, just the size of the frames. But 
its definitely worth noting because I don't quite understand how it can 
be so starkly different. In fact, that configuration across all runs
was the most diferent, again I believe thats a testament to the 
instructions per process 
